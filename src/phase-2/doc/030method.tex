We implement all the graph mining algorithms using Postgres's embedded PL/pgSQL programming language, which supports many advanced features, like user defined function, aggregate, etc. It also has a sophisticated query execution engine, which we think is the most critical component of Postgres.

The following are the algorithms we plan to implement. The reason we choose them is that there are a lot of implementation in other platform like MapReduce, we can compare our SQL version with them to draw a clusion about SQL's unique charastic in solving data analytic tasks.

\begin{description}
  \item[PageRank:] Determine the importance of every node of a graph based on its connectivity. 
  \item[Connected Components:] Partition the nodes of a graph also based on its connectivity.
  \item[Radius of Every Node:] Compute the radius of every node in a graph. The radius is defined as the number of hops that a node needs to reach to its furthest neighbor.
  \item[Belief Propagation:]
\end{description}

\subsection{Pagerank}
The algorithm of pagerank is Power method. We do matrix multiplication continusly until the change in pagerank is small. 
\begin{algorithm}
\caption{Pagerank}
\begin{algorithmic}
\STATE{Bulk load graph into an edge table in database.}
\STATE{Initialization(damper factor=0.85, max iteration = 100, epsilon = 0.0001)}
\STATE{Build a weight matrix trans, initialize pagerank p}
\REPEAT
\STATE{For each node i, update its pagerank with its old value and its income node's pagerank.}
\UNTIL{Convergence}
\end{algorithmic}
\end{algorithm}

\subsection{Weakly connected components}
In terms of the implementation of weakly connected components. We borrow the idea of HCC method from the "PEGASUS" paper.\cite{Kang09}
The algorithm can be described as following:
\begin{algorithm}
\caption{Weakly Connected Component}
\begin{algorithmic}
\STATE{Bulk load graph into an edge table in database.}
\STATE{Create a component table, where each entry contains a node id, and the component id.}
\STATE{Initialize the component table where component id equls node id.}
\REPEAT
\STATE{For each node, assign the minimum component id of its neighbors as the new componnet id of this node.}
\UNTIL{Convergence}
\end{algorithmic}
\end{algorithm}
After several rounds of iteration, the nodes in the same connected component will share the same component id.
The number of iterations for convergence can be proved to be upper bounded by the diameter of the graph.

\subsection{Radius of every node}
We discard the traditional algorithm because it is extremely infeasible for large graphs, since it uses a set to record every neghbors within n hops for a node during iteration, which requires a O($n^2$) space.
Since the exact algorithm is hopeless, we use the approximation algorithm described in "HADI" paper\cite{DBLP:journals/tkdd/KangTAFL11} instead. Specifically, we use the Flajolet-Martin algorithm for counting the number of distinct members in a multiset. It is guaranteed to give an unbiased estimate and a tight O($log(N)$) bound for space complexity. The basic idea of Floajolet-Martin algorthm is to use a bitstrings of length $L$ to encode the set. For each element to add, we randomly pick up a index according to a specified distribution, and assign BITMAP[index] to 1. Following this procedure to add element, the size of the final set can be estimated by $\frac{1}{\phi} 2^{\frac{1}{k}\sum_{i=1}^k R_i}$, where $\phi$ = 0.77351, $R_i$ denotes the index of the leftmost 0 in the the $k$th bitstring.

In our proposed method for computing radius of every node, we use the Flajolet-Martin(FM) bitstrings to encode the neighbors of every node. Formally, we use $k$ FM-bitstrings $b(h, i)$ to represent the set of neighbor nodes reachable from $node_i$ within h hops. And for each iteration, we use the following way to update each FM-bitstring:$$b(h,i) = b(h-1,i)  \quad BIT-OR \quad  {b(h-1,j)|(i,j)\in E}$$

Given the above description of how to encode neighbors of a node, and the method to update bitstrings, we can describe the approximation method we used to compute the raidus of every node in algorithm \ref{radius:algo3}.

\begin{algorithm}
\caption{Radius of Every Node}
\begin{algorithmic}
\STATE{Bulk load graph into an edge table in database.}
\STATE{Preprocess the edge table, add a self loop edge to every node in the graph.}
\STATE{Intialize the vertex table, which contains node id, and a column of bitstring array, the bitstrings are initialized using the FM algorithm}
\REPEAT
\STATE{For every node update the bitstring according to formula: $b(h,i) = b(h-1,i) \quad BIT-OR \quad  {b(h-1,j)|(i,j)\in E}$. }
\STATE{For every node, check whether the bitstrings is unchanged before and after updates, if it's not changed, output i as the radius for this node.}
\UNTIL{The bitstrings of every node stablizes or it reaches the maximum rounds of iteration.}
\end{algorithmic}
\label{radius:algo3}
\end{algorithm}


To make this algorithm possible in SQL, we defined some user defined functions:
\begin{description}
  \item[fmAssign:] Assign the k FM-bitstrings for a node.
  \item[bitOr:] Execute the OR operations between two bitstring arrays.
  \item[aggBitOr:] Aggregate function for bitOr.
  \item[fmSize:] Estimate the size of a set encoded by FM-bitstring.
\end{description}

\subsection{Belief Propagation}
For belief propagation, we use the fabp method proposed in paper\cite{DBLP:conf/pkdd/KoutraKKCPF11}.
It can be shown that the solution of belief propagation can be approximated by the linear system:$$[\mathbf{I}	 + a\mathbf{D} - c\mathbf{A}]\mathbf{b_h} = \mathbf{\phi_h}$$ 
where $\mathbf{A}$ is the n by n symmetric adjacency matrix, $\mathbf{D}$ is the diagonal matrix of degrees, $b_h$ corresponds to the vector of final beliefs for each node, $\phi_h$ is prior belief vector, and $h_h$ is the homophily factor,  $a = 4h_h^2/(1 - h_h^2)$ and $c = 2h_h / (1-4h_h^2)$.

To solve this linear system, we can see :$\mathbf{I} + a\mathbf{D} - c\mathbf{A}$ as the form $\mathbf{I} - \mathbf{W}$, where $\mathbf{W} = -a\mathbf{D} + c\mathbf{A}$, and using the expansion:$$(\mathbf{I} - \mathbf{W})^{-1} = \mathbf{I} + \mathbf{W} + \mathbf{W}^2 + \mathbf{W}^3 + ...$$

and the solution of the linear system is given by the formula:
$$\mathbf{b_h} = (\mathbf{I} - \mathbf{W}^{-1})\mathbf{\phi_h} =\mathbf{\phi_h}  + \mathbf{\phi_h} \mathbf{W} + \mathbf{\phi_h} \mathbf{W}^2 + \mathbf{\phi_h} \mathbf{W}^3 + ...$$

Given this power method, the implementation is pretty straightforward as described in algorithm \ref{bp:algo4}.
\begin{algorithm}
\caption{Belief Propagation}
\begin{algorithmic}
\STATE{Bulk load graph into an edge table in database.}
\STATE{Initialize $h_h = 0.001 $}
\STATE{Initialize the initial belief of every node as prior belief}
\REPEAT
\STATE{Update the belief of node by $b_h(i) = b_h(i-1)\mathbf{W} + \mathbf{\phi_h}$ }
\UNTIL{Convergence}
\end{algorithmic}
\label{bp:algo4}
\end{algorithm}

\subsection{Eigenvalue/SVD}
We adopt the method propose in \cite{kang2011spectral}. There are several methods to solve part of eigenvalue computation problems, for instance, power method\cite{langville2004deeper}. While it has the limitation that it can only extract the eigenvector with biggest eigenvalue. Several method have been proposed to extract top k eigenvectors simultaneously. The approach we use is Lanczos algorithm\cite{lanczos1950iteration}. The general idea about this algorithm is that instead of directly work on an $N \times N$ matrix, we first generate a skinny $N \times m$ matrix(M $\ll$ N). Then it computes a small $M \times M$ dense matrix which has good approximation to the eigenvalues of the original matrix. In this case, we directly apply quadratic algorithm to top-k eigenvalues. Notice that k $<$ M.

{\bf{Algorithm detail:}} Different from power method, the intermediate multiplication matrix is used to construct a set of orthonormal base of \emph{Krylv subspace $K_{m}$} which follows the definition:
\begin{equation}
K_{m} = < b, Ab, \cdots, A^{m-1}b>.
\end{equation}
The sub procedure to construct orthonormal bases may be any standard algorithm, for example Gram-schmidt algorithm. We can view Lanczos algorithm as an iterative method which incrementally construct Krylov subspace. The pseudo-code is shown in Algorithm \ref{eigen:algo1}.

After Lanczos factorization, we get a few matrices that satisfy the following equation:
\begin{equation}
	AV_{m} = V_{m}T_{m} + f_{m}e^{T}_{m}
\end{equation}
To name a few, $A^{n\times m}$ is input matrix, $V^{n\times m}_{m}$ contains the m orthonormal bases, $T^{m\times m}_{m}$ is a tridiagonal matrix, $f_{m}$ is new n-vector orthogonal to all columns of $V_{m}$, $
e_{m}$ is a vector that \emph{m}th element is 1, and others 0. After algorithm \ref{eigen:algo1}, we need to construct the matrix $T^{m\times m}_{m}$. The algorithm is quite simple, it is listed in algorithm \ref{eigen:algo2}. 

The eigenvalues of $T_{m}$ are called Ritz values, and $V_{m}Y$'s columns are called Ritz vector. It is constructed by Algorithm \ref{eigen:algo3}. We expect the Ritz values and Ritz vectors to be good approximation of the eigenvalues and eigenvectors of original matrix. The computation of eigenvalues of $T_{m}$ can be done by standard quadratic algorithms, such as QR method. 

\begin{algorithm}
{\bf Input:} Matrix $A^{n \times m}$\\
random n-vector $b$,\\
number of steps m\\
{\bf output:} Orthogonal matrix $ V^{v \times m}_{m} = [v_{1}\cdots v{m}]$,\\
coefficients $\alpha[1..m]$ and $\beta[1..m-1]$
\begin{algorithmic}[1]
\caption{Lanczos algorithm}
\STATE $\beta_{0} \leftarrow 0, v_{0} \leftarrow 0, v_{1} \leftarrow \frac{b}{\parallel b \parallel}$ 
\FOR {$i=1$ to $m$}
	\STATE $v \leftarrow Av_{i}$
	\STATE $\alpha_{i} \leftarrow v^{T}_{i}v $
	\STATE $v \leftarrow v - \beta_{i-1}v_{i-1} - \alpha_{i}v_{i}$
	\STATE $\beta_{i} \leftarrow \parallel v\parallel $
	\IF {$\beta_{i} = 0$} 
	\STATE break for loop 
	\ENDIF
	\STATE $ v_{v+1} \leftarrow \frac{v}{\beta_{i}} $
\ENDFOR
\end{algorithmic}
\label{eigen:algo1}
\end{algorithm}

\begin{algorithm}
\caption{Build tridiagonal matrix}
{\bf Input:} $\alpha, \beta$
{\bf Output:} $T^{m\times m}_{m}$
\begin{algorithmic}[1]
\FOR {$i=1$ to m}
	\STATE $T[i, i] \leftarrow \alpha_{i} $
	\STATE $T[i, i+1] = T[i+1, i] \leftarrow \beta_{i}$
\ENDFOR	
\end{algorithmic}
\label{eigen:algo2}
\end{algorithm}

\begin{algorithm}
\caption{Compute Ritz values}
{\bf Input:}Orthogonal matrix $V^{n\times m}_{m}$\\
coefficients $\alpha[1..m]$ and $\beta[1..m-1]$
\begin{algorithmic}[1]
\STATE $T_{m} \leftarrow$ (build a tridiagonal matrix from $\alpha$ and $\beta$)
\STATE $QDQ^{T} \leftarrow EIG(T_{m})$
\STATE $\lambda_{1..k} \leftarrow$ (top k eigenvalues from D)
\STATE $Q_{k} \leftarrow $ (k columns of Q corresponding to $\lambda_{1..k})$
\STATE $R_{k} \leftarrow V_{m}Q_{k}$
\end{algorithmic}
\label{eigen:algo3}
\end{algorithm}

