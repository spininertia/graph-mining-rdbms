\subsection{Papers read by Wei Chen}
The first paper was "Mining Large Graphs: Algorithms, Inference, and Discoveries".
\cite{DBLP:conf/icde/KangCF11}
\begin{itemize*}
\item {\em Main idea}: 
This paper discuss how to do inference in graphical model under distributed setting, what if graph can not fit into main memory. They propose a variant of Belief Propagation called \emph{Line Graph Fixed Point(LFP)} to address this issue. The key step is to induce a new graph called \emph{Directed Line Graph} L(G) from original graph, it flips the definition of node and edge. Each node of L(G) represents an edge of G and edge exists between node $n_{i}$ and $n_j$ if and only if their related edges are incident. L(G) has a nice property that it's easy to derive exact recursive equation for it, actually the message updating equation of original Belief Propagation \cite{bp} can be adopted without modification. Then the author generalize mesage updating procedure as general Matrix-Vector operations, namely combine2(), combineAll(), sumVector() and assign() from \cite{Kang09}. While directly apply LFP without careful design will lead to drastic storage requirement to store L(G), the idea to tackle this is \emph{Lazy Multiplication}, all operations on L(G) is is actually run on G. They propose an algorithm employing this idea, and run on Hadoop, its name is \emph{Hadoop Line Graph Fixed Point}. Ha-LFP solves the biggest issue of the most graph mining algorithm --- scalability. Since Ha-LFP is based on Hadoop, it inherits fault tolerance, data replication natively.
\item {\em Use for our project}:
      LFP seems like a promising method to do Belief Propagation, combine2, combineAll, assign is natural in SQL. Matrix operation is tractable in SQL, and can be optimized by RBDMS. The performance could be comparable to Hadoop version. Also we don't need to construct L(G) explicitly, this avoids expensive cost in storage and time to access and update L(G).
\item {\em Shortcomings}:
      It's kind of tricky to say this algorithm solve scalability perfectly. I think this is more contributed to Hadoop instead of LFP. There may exist better formulation of Belief Propagation on Hadoop. The experiments are conducted on M45, which is one of the most advanced supercomputer.
\end{itemize*}


The second paper was "Understanding Belief Propagation and its Generalizations".
\cite{bp}
\begin{itemize*}
\item {\em Main idea}: Marginal Probability is important in graphical model inferencing, while most of the time it is expensive to calculate, the running time is usually exponential to the number of nodes. \emph{Belief Propagation} is a neat technique to bring the running time down to linear to the number of edges in graphical model, and it has been widely used beyong machine learning, like computer vision\cite{felzenszwalb2006efficient}, turbo code 
\cite{mceliece1998turbo}, etc. The success of Belief Propagation results from a property called \emph{Conidtional Independence}. Most graphical models have the property that there exist many conditional independence between nodes. The core idea of Belief Propagation is to exploit this property to decouple the computation of global probability into several load computations. The marginal probability of node $n_{i}$ is called \emph{belief}. Each node $n_{i}$ sends messages to other nodes $n_{j}$. \emph{Message} $m_{ij}(n_{j})$ represents how node $n_{i}$ thinks node $n_{j}$'s state should be. A node can calculate its belief once it has received messages from all its neighbouts. So each edge is associate with two messages. The paper shows that each message can be calculated only once, thus we can use dynamic programming to compute every node's belief in linear time. The paper also introduce Generalized Belief Propagation which extends BP to group of nodes.
\item {\em Use for our project}:
      This paper discusses how to compute belief of each node in one round(for trees). We can first implement the algorithms mentioned in this paper as a baseline. It is also helpful for understanding other optimizing algorithms for Belief Propagation. 
\item {\em Shortcomings}:
       There is not much discussion about the implementation in SQL. But user defined functions provided by PostgreSQL has limited expressiveness, so the real implementation will be quite different.
\end{itemize*}


The third paper was "A Comparison of Approaches to Large-Scale Data Analysis".
\cite{pavlo2009comparison}
\begin{itemize*}
\item {\em Main idea}: 
    This paper compares the tradeoff between Parallel DBMS and MapReduce framework in large data analysis. The authors want to advocate the capability of Parallel DBMS in implementing large scale analytic tasks. The paper analyze the fundamental difference of Parallel DBMS and MapReduce, their support in schema, data distribution, fault tolerance, index, and utility tools. Then they conduct experiments about their performance in different tasks, like pattern searching, aggregation, join, etc. They conclude that Parallel DBMS outperforms MapReduce in all kinds of tasks. But Parallel DBMS requires a lot of effort to configure and profile to reach good performance, MapReduce is relatively much easier to use. In the end, the paper admit MapReduce's advantage over complex Parallel DBMS, while still suggest Parallel DBMS as an option for specific tasks.
\item {\em Use for our project}:
    We can borrow some ideas from theirs experiments about investigate SQL function. Their analysis method is also a good model to follow.
\item {\em Shortcomings}:
    The tasks they experimented are kind of trivial. We don't know the real difference in when the systems are used for more complex graph mining algorithms.
\end{itemize*}

The fourth paper was "Pig Latin: A Not-So-Foreign Language for Data Processing".
\cite{olston2008pig}
\begin{itemize*}
\item {\em Main idea}: 
    Either RDBMS or MapReduce framework represents some extreme in large scale data analysis, either unnatural for programmers' mind(SQL) or too low level to express many algorithms(MapReduce). Different solutions are proposed. Pig is a platform of Yahoo that try to reach a balance between these two styles to support more flexible(ad-hoc) data analysis tasks. It takes a hybrid strategy, provides friendly user interface(programming, debugging), while actual tasks are running on powerful Hadoop platform without loss of performance. And it propose a new high level programming language called \emph{Pig Latin} that gives user more control of data flow, which programmer can adopt an imperative style, while retain the expressiveness of SQL. It also support several key operations that can be automatically parallelized, like filter, cogroup, group, join, etc. Its underlying implementation is compile Pig Latin programs into Hadoop jobs, so that it is also equipped with the benefits of MapReduce. 
\item {\em Use for our project}:
    Pig Latin's SQL-like syntax provides finer grained control over dataflow, which is more suitable for implementing graph mining algorithms. We can compare with SQL user defined function implementation of these algorithms, and analyze their intrinsic data accessing characteristic.
\item {\em Shortcomings}:
    There are few graph mining algorithms implementation in Pig Latin, only a few open source analysis package like Linkedin's Datafu. And Pig only supports read-only data analysis workloads, which may lead to many unnecessary efforts if we want to do extensive write during computation.
\end{itemize*}
